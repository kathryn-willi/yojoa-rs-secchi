---
title: "xgboost algorithm for Secchi - high data stringency"
author: "B Steele w/Edits from Matt Ross"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, comment = FALSE, message = FALSE,
                      cache = FALSE)

library(tidyverse)
library(xgboost)
library(Metrics)
library(ggpmisc)
library(ggthemes)

match_dir = 'data/matchups/'
model_dir = 'data/models/'

# Set random seed
set.seed(799)

```

# Purpose

The purpose of this script is to apply the `xgboost` algorithm to Remote
Sensing Imagery of Lake Yojoa in Honduras, to estimate Yojoa water
clarity. You can read more about this lake
[here](https://www.sciencedirect.com/science/article/pii/S0048969722015479).

In this very stringent approach, we subset the Secchi-Landsat matchups
by Landsat flyover, resulting in no image-date crossover between the
train, test, or validation datasets. 

## Load matchup data

Note, we do not use the 'weight' column in this analysis, but create it here for the next script algorithm development, `10_xgboost_higher_secchi`.

```{r}
#list all the files in the match directory
match = list.files(match_dir)

prepData = function(df) {
  #make a rowid column
  df_prep = df %>% 
    rowid_to_column() %>% 
    mutate(secchi = as.numeric(secchi)) %>% #there's one wonky value in here with two decimal points... dropping from this analysis
    filter(!is.na(secchi))
}


#load the matchup files 
threeDay = read.csv(file.path(match_dir, match[grepl('three', match) & !grepl('us', match)])) %>%
  prepData(.) %>% 
  mutate(weight = secchi/max(secchi)*10)
fiveDay = read.csv(file.path(match_dir, match[grepl('five', match) & !grepl('us', match)])) %>%
  prepData(.) %>% 
  mutate(weight = secchi/max(secchi)*10)
multiMatch = read.csv(file.path(match_dir, match[grepl('multi', match)])) %>% 
  prepData(.) %>% 
  mutate(weight = secchi/max(secchi)*10)

```

We want to predict the `secchi` value in these datasets, so let's set
the `target` as that variable:

```{r}
## Identify our target (value is secchi)
target <- 'secchi'
```

# xgboost Runs

## Make test, training, validation sets by image date

60% of the data as the 'train' set, and split the remainder between
'test' and 'val'.

## For the 3-day set

```{r}
system.index = unique(threeDay$system.index)

##Pull 60% as training data
train_3_uid <- tibble(system.index) %>%
  sample_frac(.6) 

test_3_uid <- tibble(system.index) %>%
  anti_join(., train_3_uid) %>%
  sample_frac(0.5)

val_3_uid <-  tibble(system.index) %>%
  anti_join(., train_3_uid) %>% 
  anti_join(., test_3_uid)

train_3 <- train_3_uid %>% 
  left_join(., threeDay) %>% 
  data.frame(.)

test_3 <- test_3_uid %>% 
  left_join(., threeDay)%>% 
  data.frame(.)

val_3 <- val_3_uid %>% 
  left_join(., threeDay)%>% 
  data.frame(.)
```

## For the 5-day set

```{r}
system.index = unique(fiveDay$system.index)

##Pull 60% as training data
train_5_uid <- tibble(system.index) %>%
  sample_frac(.6) 

test_5_uid <- tibble(system.index) %>%
  anti_join(., train_5_uid) %>%
  sample_frac(0.5)

val_5_uid <-  tibble(system.index) %>%
  anti_join(., train_5_uid) %>% 
  anti_join(., test_5_uid)

train_5 <- train_5_uid %>% 
  left_join(., fiveDay) %>% 
  data.frame(.)

test_5 <- test_5_uid %>% 
  left_join(., fiveDay)%>% 
  data.frame(.)

val_5 <- val_5_uid %>% 
  left_join(., fiveDay)%>% 
  data.frame(.)
```

## For the Local Knowledge Window

The 'local knowledge' window creates a variable matchup window, where
matchups can be within 7 days in all times of year except October and
November, when the lake can have dramatic shifts in clarity in a short
period of time.

```{r}
system.index = unique(multiMatch$system.index)

##Pull 60% as training data
train_j_uid <- tibble(system.index) %>%
  sample_frac(.6) 

test_j_uid <- tibble(system.index) %>%
  anti_join(., train_j_uid) %>%
  sample_frac(0.5)

val_j_uid <-  tibble(system.index) %>%
  anti_join(., train_j_uid) %>% 
  anti_join(., test_j_uid)

train_j <- train_j_uid %>% 
  left_join(., multiMatch) %>% 
  data.frame(.)

test_j <- test_j_uid %>% 
  left_join(., multiMatch)%>% 
  data.frame(.)

val_j <- val_j_uid %>% 
  left_join(., multiMatch)%>% 
  data.frame(.)
```

To compare apples-to-apples between this and the next analysis, we'll save these train/test sets.

```{r}
save(train_3, test_3, val_3,
     train_5, test_5, val_5,
     train_j, test_j, val_j,
     file = 'data/models/train_test_val_verystringent.RData')
```

## Name feature groups

Here, we indicate the features to be used in our models. We'll use the
visual bands and add in summaries of ERA5 met data. In our datasets, the
5-day met summaries have the suffix '\_5', etc. The first two models only include recent weather summaries, but the final two models include 5 or 7 day weather summaries as well as the previous day's weather.

```{r}
band_met3_feats <-  c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr',
                     'RN', 'BG', 'RB','GB',
                     'tot_sol_rad_KJpm2_3', 'max_temp_degK_3', 'mean_temp_degK_3', 'min_temp_degK_3',
                     'tot_precip_m_3', 'mean_wind_mps_3')

band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr',
                     'RN', 'BG', 'RB','GB',
                     'tot_sol_rad_KJpm2_5', 'max_temp_degK_5', 'mean_temp_degK_5', 'min_temp_degK_5',
                     'tot_precip_m_5', 'mean_wind_mps_5')

band_met51_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr',
                     'RN', 'BG', 'RB','GB',
                     'tot_sol_rad_KJpm2_5', 'max_temp_degK_5', 'mean_temp_degK_5', 'min_temp_degK_5',
                     'tot_precip_m_5', 'mean_wind_mps_5',
                     'solar_rad_KJpm2_prev', 'precip_m_prev','air_temp_degK_prev','wind_speed_mps_prev')

band_met71_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr',
                     'RN', 'BG', 'RB','GB',
                     'tot_sol_rad_KJpm2_7', 'max_temp_degK_7', 'mean_temp_degK_7', 'min_temp_degK_7',
                     'tot_precip_m_7', 'mean_wind_mps_7',
                     'solar_rad_KJpm2_prev', 'precip_m_prev','air_temp_degK_prev','wind_speed_mps_prev')
```

## Format data for xgboost

### 3 Day Window

```{r}
## 3 day window, 3 days previous met
dtrain_3d_3m <- xgb.DMatrix(data = as.matrix(train_3[,band_met3_feats]), 
                            label = train_3[,target])

dtest_3d_3m <- xgb.DMatrix(data = as.matrix(test_3[,band_met3_feats]), 
                     label = test_3[,target])

dval_3d_3m <- xgb.DMatrix(data = as.matrix(val_3[,band_met3_feats]), 
                     label = val_3[,target])

## 3 day window, 5 days previous met
dtrain_3d_5m <- xgb.DMatrix(data = as.matrix(train_3[,band_met5_feats]), 
                            label = train_3[,target])

dtest_3d_5m <- xgb.DMatrix(data = as.matrix(test_3[,band_met5_feats]), 
                     label = test_3[,target])

dval_3d_5m <- xgb.DMatrix(data = as.matrix(val_3[,band_met5_feats]), 
                     label = val_3[,target])

## 3 day window, 5/1 days previous met
dtrain_3d_51m <- xgb.DMatrix(data = as.matrix(train_3[,band_met51_feats]), 
                            label = train_3[,target])

dtest_3d_51m <- xgb.DMatrix(data = as.matrix(test_3[,band_met51_feats]), 
                     label = test_3[,target])

dval_3d_51m <- xgb.DMatrix(data = as.matrix(val_3[,band_met51_feats]), 
                     label = val_3[,target])

## 3 day window, 7/1 days previous met
dtrain_3d_71m <- xgb.DMatrix(data = as.matrix(train_3[,band_met71_feats]), 
                            label = train_3[,target])

dtest_3d_71m <- xgb.DMatrix(data = as.matrix(test_3[,band_met71_feats]), 
                     label = test_3[,target])

dval_3d_71m <- xgb.DMatrix(data = as.matrix(val_3[,band_met71_feats]), 
                     label = val_3[,target])


```

### 5 Day Window

```{r}
## 5 day window, 3 days previous met
dtrain_5d_3m <- xgb.DMatrix(data = as.matrix(train_5[,band_met3_feats]), 
                            label = train_5[,target])

dtest_5d_3m <- xgb.DMatrix(data = as.matrix(test_5[,band_met3_feats]), 
                     label = test_5[,target])

dval_5d_3m <- xgb.DMatrix(data = as.matrix(val_5[,band_met3_feats]), 
                     label = val_5[,target])

## 5 day window, 5 days previous met
dtrain_5d_5m <- xgb.DMatrix(data = as.matrix(train_5[,band_met5_feats]), 
                            label = train_5[,target])

dtest_5d_5m <- xgb.DMatrix(data = as.matrix(test_5[,band_met5_feats]), 
                     label = test_5[,target])

dval_5d_5m <- xgb.DMatrix(data = as.matrix(val_5[,band_met5_feats]), 
                     label = val_5[,target])

## 5 day window, 5/1 days previous met
dtrain_5d_51m <- xgb.DMatrix(data = as.matrix(train_5[,band_met51_feats]), 
                            label = train_5[,target])

dtest_5d_51m <- xgb.DMatrix(data = as.matrix(test_5[,band_met51_feats]), 
                     label = test_5[,target])

dval_5d_51m <- xgb.DMatrix(data = as.matrix(val_5[,band_met51_feats]), 
                     label = val_5[,target])

## 5 day window, 7/1 days previous met
dtrain_5d_71m <- xgb.DMatrix(data = as.matrix(train_5[,band_met71_feats]), 
                            label = train_5[,target])

dtest_5d_71m <- xgb.DMatrix(data = as.matrix(test_5[,band_met71_feats]), 
                     label = test_5[,target])

dval_5d_71m <- xgb.DMatrix(data = as.matrix(val_5[,band_met71_feats]), 
                     label = val_5[,target])


```

### Local Knowledge Window

```{r}
## local knowledge window, 5 days previous met
dtrain_jd_5m <- xgb.DMatrix(data = as.matrix(train_j[,band_met5_feats]), 
                            label = train_j[,target])

dtest_jd_5m <- xgb.DMatrix(data = as.matrix(test_j[,band_met5_feats]), 
                     label = test_j[,target])

dval_jd_5m <- xgb.DMatrix(data = as.matrix(val_j[,band_met5_feats]), 
                     label = val_j[,target])

## local knowledge window, 3 days previous met
dtrain_jd_3m <- xgb.DMatrix(data = as.matrix(train_j[,band_met3_feats]), 
                            label = train_j[,target])

dtest_jd_3m <- xgb.DMatrix(data = as.matrix(test_j[,band_met3_feats]), 
                     label = test_j[,target])

dval_jd_3m <- xgb.DMatrix(data = as.matrix(val_j[,band_met3_feats]), 
                     label = val_j[,target])

## local knowledge window, 5/1 days previous met
dtrain_jd_51m <- xgb.DMatrix(data = as.matrix(train_j[,band_met51_feats]), 
                            label = train_j[,target])

dtest_jd_51m <- xgb.DMatrix(data = as.matrix(test_j[,band_met51_feats]), 
                     label = test_j[,target])

dval_jd_51m <- xgb.DMatrix(data = as.matrix(val_j[,band_met51_feats]), 
                     label = val_j[,target])

## local knowledge window, 7/1 days previous met
dtrain_jd_71m <- xgb.DMatrix(data = as.matrix(train_j[,band_met71_feats]), 
                            label = train_j[,target])

dtest_jd_71m <- xgb.DMatrix(data = as.matrix(test_j[,band_met71_feats]), 
                     label = test_j[,target])

dval_jd_71m <- xgb.DMatrix(data = as.matrix(val_j[,band_met71_feats]), 
                     label = val_j[,target])

```

### Parameter optimization

This is an xgboost optimization method developed by Sam Sillen where you
list many possible hyperparameter options and then create a matrix of
all possible combinations - aka 'grid search' - and grab the top 20 performing combinations of
hyperparameters by square error (our loss statistic). 

```{r}
# Hypertune xgboost parameters and save as 'best_params' 
grid_train <- expand.grid(
  max_depth= c(3,6,8),
  subsample = c(.5,.8,1),
  colsample_bytree= c(.5,.8,1),
  eta = c(0.1, 0.3),
  min_child_weight= c(3,5,7)
)

hypertune_xgboost = function(train,test, grid){
  params <- list(booster = "gbtree", objective = 'reg:squarederror', 
                 eta=grid$eta ,max_depth=grid$max_depth, 
                 min_child_weight=grid$min_child_weight,
                 subsample=grid$subsample, 
                 colsample_bytree=grid$colsample_bytree)
  xgb.naive <- xgb.train(params = params, data = train, nrounds = 1000, 
                         watchlist = list(train = train, val = test), 
                         verbose = 0,
                         early_stopping_rounds = 20)
  summary <- grid %>% mutate(val_loss = xgb.naive$best_score, best_message = xgb.naive$best_msg,
                             mod = list(xgb.naive))
  
  return(summary) 
}
```

### 3 day window, 3 day met hypertuning

Note, evaluation is turned off for these chunks as to not overwrite
previous models and parameter tuning in next section.

```{r, eval = F}
## Hypertune xgboost 3 day window, 3 day met
xgboost_hypertune_3d_3m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_3d_3m,dtest_3d_3m,current)
  })

mod_summary_3d_3m <- xgboost_hypertune_3d_3m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_3d_3m <- xgboost_hypertune_3d_3m[xgboost_hypertune_3d_3m$val_loss==min(xgboost_hypertune_3d_3m$val_loss),]

save(mod_summary_3d_3m,best_mod_3d_3m, file = 'data/models/paramsxg_ep_val_3d_3m.RData')
```

### 3 day window, 5 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 3 day window, 5 day met
xgboost_hypertune_3d_5m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_3d_5m,dtest_3d_5m,current)
  })

mod_summary_3d_5m <- xgboost_hypertune_3d_5m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_3d_5m <- xgboost_hypertune_3d_5m[xgboost_hypertune_3d_5m$val_loss==min(xgboost_hypertune_3d_5m$val_loss),]

save(mod_summary_3d_5m,best_mod_3d_5m, file = 'data/models/paramsxg_ep_val_3d_5m.RData')

```

### 3 day window, 5/1 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 5 day window, 5/1 day met
xgboost_hypertune_3d_51m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_3d_51m,dtest_3d_51m,current)
  })

mod_summary_3d_51m <- xgboost_hypertune_3d_51m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_3d_51m <- xgboost_hypertune_3d_51m[xgboost_hypertune_3d_51m$val_loss==min(xgboost_hypertune_3d_51m$val_loss),]

save(mod_summary_3d_51m,best_mod_3d_51m, file = 'data/models/paramsxg_ep_val_3d_51m.RData')
```

### 3 day window, 7/1 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 5 day window, 7/1 day met
xgboost_hypertune_3d_71m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_3d_71m,dtest_3d_71m,current)
  })

mod_summary_3d_71m <- xgboost_hypertune_3d_71m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_3d_71m <- xgboost_hypertune_3d_71m[xgboost_hypertune_3d_71m$val_loss==min(xgboost_hypertune_3d_71m$val_loss),]

save(mod_summary_3d_71m,best_mod_3d_71m, file = 'data/models/paramsxg_ep_val_3d_71m.RData')
```

### 5 day window, 3 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 5 day window, 3 day met
xgboost_hypertune_5d_3m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_5d_3m,dtest_5d_3m,current)
  })

mod_summary_5d_3m <- xgboost_hypertune_5d_3m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_5d_3m <- xgboost_hypertune_5d_3m[xgboost_hypertune_5d_3m$val_loss==min(xgboost_hypertune_5d_3m$val_loss),]

save(mod_summary_5d_3m,best_mod_5d_3m, file = 'data/models/paramsxg_ep_val_5d_3m.RData')
```

### 5 day window, 5 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 5 day window, 5 day met
xgboost_hypertune_5d_5m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_5d_5m,dtest_5d_5m,current)
  })

mod_summary_5d_5m <- xgboost_hypertune_5d_5m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_5d_5m <- xgboost_hypertune_5d_5m[xgboost_hypertune_5d_5m$val_loss==min(xgboost_hypertune_5d_5m$val_loss),]

save(mod_summary_5d_5m,best_mod_5d_5m, file = 'data/models/paramsxg_ep_val_5d_5m.RData')

```

### 5 day window, 5/1 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 5 day window, 5/1 day met
xgboost_hypertune_5d_51m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_5d_51m,dtest_5d_51m,current)
  })

mod_summary_5d_51m <- xgboost_hypertune_5d_51m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_5d_51m <- xgboost_hypertune_5d_51m[xgboost_hypertune_5d_51m$val_loss==min(xgboost_hypertune_5d_51m$val_loss),]

save(mod_summary_5d_51m,best_mod_5d_51m, file = 'data/models/paramsxg_ep_val_5d_51m.RData')
```

### 5 day window, 7/1 day met hypertuning

```{r, eval = F}
## Hypertune xgboost 5 day window, 7/1 day met
xgboost_hypertune_5d_71m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_5d_71m,dtest_5d_71m,current)
  })

mod_summary_5d_71m <- xgboost_hypertune_5d_71m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_5d_71m <- xgboost_hypertune_5d_71m[xgboost_hypertune_5d_71m$val_loss==min(xgboost_hypertune_5d_71m$val_loss),]

save(mod_summary_5d_71m,best_mod_5d_71m, file = 'data/models/paramsxg_ep_val_5d_71m.RData')
```

### Local knowledge window, 3 day met

```{r, eval = F}
## Hypertune xgboost local knowledge window, 3 day met
xgboost_hypertune_jd_3m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_jd_3m,dtest_jd_3m,current)
  })

mod_summary_jd_3m <- xgboost_hypertune_jd_3m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_jd_3m <- xgboost_hypertune_jd_3m[xgboost_hypertune_jd_3m$val_loss==min(xgboost_hypertune_jd_3m$val_loss),]

save(mod_summary_jd_3m,best_mod_jd_3m, file = 'data/models/paramsxg_ep_val_jd_3m.RData')
```

### Local knowledge window, 5 day met

```{r, eval = F}
## Hypertune xgboost local knowledge window, 5 day met
xgboost_hypertune_jd_5m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_jd_5m,dtest_jd_5m,current)
  })

mod_summary_jd_5m <- xgboost_hypertune_jd_5m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_jd_5m <- xgboost_hypertune_jd_5m[xgboost_hypertune_jd_5m$val_loss==min(xgboost_hypertune_jd_5m$val_loss),]

save(mod_summary_jd_5m,best_mod_jd_5m, file = 'data/models/paramsxg_ep_val_jd_5m.RData')
```

### Local knowledge window, 5/1 day met

```{r, eval = F}
## Hypertune xgboost local knowledge window, 5/1 day met
xgboost_hypertune_jd_51m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_jd_51m,dtest_jd_51m,current)
  })

mod_summary_jd_51m <- xgboost_hypertune_jd_51m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_jd_51m <- xgboost_hypertune_jd_51m[xgboost_hypertune_jd_51m$val_loss==min(xgboost_hypertune_jd_51m$val_loss),]

save(mod_summary_jd_51m,best_mod_jd_51m, file = 'data/models/paramsxg_ep_val_jd_51m.RData')
```

### Local knowledge window, 7/1 day met

```{r, eval = F}
## Hypertune xgboost local knowledge window, 7/1 day met
xgboost_hypertune_jd_71m <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain_jd_71m,dtest_jd_71m,current)
  })

mod_summary_jd_71m <- xgboost_hypertune_jd_71m %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod_jd_71m <- xgboost_hypertune_jd_71m[xgboost_hypertune_jd_71m$val_loss==min(xgboost_hypertune_jd_71m$val_loss),]

save(mod_summary_jd_71m,best_mod_jd_71m, file = 'data/models/paramsxg_ep_val_jd_71m.RData')
```

# Model Assessment and Application

### load model summaries

```{r}
load('data/models/paramsxg_ep_val_3d_5m.RData')
load('data/models/paramsxg_ep_val_3d_3m.RData')
load('data/models/paramsxg_ep_val_3d_51m.RData')
load('data/models/paramsxg_ep_val_3d_71m.RData')
load('data/models/paramsxg_ep_val_5d_5m.RData')
load('data/models/paramsxg_ep_val_5d_3m.RData')
load('data/models/paramsxg_ep_val_5d_51m.RData')
load('data/models/paramsxg_ep_val_5d_71m.RData')
load('data/models/paramsxg_ep_val_jd_5m.RData')
load('data/models/paramsxg_ep_val_jd_3m.RData')
load('data/models/paramsxg_ep_val_jd_51m.RData')
load('data/models/paramsxg_ep_val_jd_71m.RData')

```

Now that these are loaded, we need to look at the test/train statistics.
Ideally the train/test RMSE are relatively close so we don't choose too
overfit of a model. Below, we apply the model to the validation dataset and plot the validation observed versus predicted.

The optimized booster was chosen as the highest-ranked booster where the train/test were within 0.15m RMSE. If no booster met that, the booster with the closest train/test RMSE was chosen.

### Three day window dataset

```{r}
mod_summary_3d_3m %>% select(best_message)
mod_summary_3d_5m %>% select(best_message)
mod_summary_3d_51m %>% select(best_message)
mod_summary_3d_71m %>% select(best_message)
```

```{r}
# most of best models are overfit, so looking for train/test RMSE that are closer
optimized_booster_3d_3m <- mod_summary_3d_3m$mod[4][[1]]
optimized_booster_3d_5m <- mod_summary_3d_5m$mod[6][[1]]
optimized_booster_3d_51m <- mod_summary_3d_51m$mod[17][[1]]
optimized_booster_3d_71m <- mod_summary_3d_71m$mod[16][[1]]
#note that pretty much all of these are overfit.

# Apply best mod
preds_3 <- val_3 %>% 
  mutate(pred_secchi_3d_5m = predict(optimized_booster_3d_5m, dval_3d_5m),
         pred_secchi_3d_3m = predict(optimized_booster_3d_3m, dval_3d_3m),
         pred_secchi_3d_51m = predict(optimized_booster_3d_51m, dval_3d_51m),
         pred_secchi_3d_71m = predict(optimized_booster_3d_71m, dval_3d_71m))

evals_3 <- preds_3 %>%
  summarise(across(c(pred_secchi_3d_3m, pred_secchi_3d_5m, pred_secchi_3d_71m, pred_secchi_3d_51m),
                   list(rmse = ~rmse(secchi, .),
                        mae = ~mae(secchi, .),
                        mape = ~mape(secchi, .),
                        bias = ~bias(secchi, .),
                        p.bias = ~percent_bias(secchi, .),
                        smape = ~smape(secchi, .),
                        r2 = ~cor(secchi, .)^2), 
                   .names = "{fn}_{col}"))

evals_3
```

### Five day window dataset

```{r}
mod_summary_5d_3m %>% select(best_message)
mod_summary_5d_5m %>% select(best_message)
mod_summary_5d_51m %>% select(best_message)
mod_summary_5d_71m %>% select(best_message)
```

```{r}
# most of best models are overfit, so looking for train/test RMSE that are closer
optimized_booster_5d_3m <- mod_summary_5d_3m$mod[2][[1]]
optimized_booster_5d_5m <- mod_summary_5d_5m$mod[7][[1]]
optimized_booster_5d_51m <- mod_summary_5d_51m$mod[14][[1]]
optimized_booster_5d_71m <- mod_summary_5d_71m$mod[13][[1]]
# none of the train/test rmse's are particularly close

# Apply best mod
preds_5 <- val_5 %>% 
  mutate(pred_secchi_5d_5m = predict(optimized_booster_5d_5m, dval_5d_5m),
         pred_secchi_5d_3m = predict(optimized_booster_5d_3m, dval_5d_3m),
         pred_secchi_5d_51m = predict(optimized_booster_5d_51m, dval_5d_51m),
         pred_secchi_5d_71m = predict(optimized_booster_5d_71m, dval_5d_71m))

evals_5 <- preds_5 %>%
  summarise(across(c(pred_secchi_5d_3m, pred_secchi_5d_5m, pred_secchi_5d_71m,pred_secchi_5d_51m),
                   list(rmse = ~rmse(secchi, .),
                        mae = ~mae(secchi, .),
                        mape = ~mape(secchi, .),
                        bias = ~bias(secchi, .),
                        p.bias = ~percent_bias(secchi, .),
                        smape = ~smape(secchi, .),
                        r2 = ~cor(secchi, .)^2), 
                   .names = "{fn}_{col}"))

evals_5
```

### Local knowledge window dataset

```{r}
mod_summary_jd_3m %>% select(best_message)
mod_summary_jd_5m %>% select(best_message)
mod_summary_jd_51m %>% select(best_message)
mod_summary_jd_71m %>% select(best_message)
```

```{r}
optimized_booster_jd_3m <- mod_summary_jd_3m$mod[3][[1]]
optimized_booster_jd_5m <- mod_summary_jd_5m$mod[1][[1]]
optimized_booster_jd_51m <- mod_summary_jd_51m$mod[14][[1]]
optimized_booster_jd_71m <- mod_summary_jd_71m$mod[9][[1]]

# Apply best mod
preds_jd <- val_j %>% 
  mutate(pred_secchi_jd_5m = predict(optimized_booster_jd_5m, dval_jd_5m),
         pred_secchi_jd_3m = predict(optimized_booster_jd_3m, dval_jd_3m),
         pred_secchi_jd_51m = predict(optimized_booster_jd_51m, dval_jd_51m),
         pred_secchi_jd_71m = predict(optimized_booster_jd_71m, dval_jd_71m))

evals_jd <- preds_jd %>%
  summarise(across(c(pred_secchi_jd_3m, pred_secchi_jd_5m, pred_secchi_jd_71m, pred_secchi_jd_51m),
                   list(rmse = ~rmse(secchi, .),
                        mae = ~mae(secchi, .),
                        mape = ~mape(secchi, .),
                        bias = ~bias(secchi, .),
                        p.bias = ~percent_bias(secchi, .),
                        smape = ~smape(secchi, .),
                        r2 = ~cor(secchi, .)^2), 
                   .names = "{fn}_{col}"))

evals_jd
```

### Model Performance - 3 day window

Keep in mind that all of these models seemed overfit in the train/test sets.


```{r, echo = F}
ggplot(preds_3, aes(x = secchi, y = pred_secchi_3d_3m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nthree day matchups, band and 3-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_3, aes(x = secchi, y = pred_secchi_3d_5m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nthree day matchups, band and 5-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_3, aes(x = secchi, y = pred_secchi_3d_51m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nthree day matchups, band and 5/1-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_3, aes(x = secchi, y = pred_secchi_3d_71m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nthree day matchups, band and 7/1-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

These all perform pretty poorly.


### Model Performance - 5 day window

Keep in mind that all of these models seemed overfit in the train/test sets.

```{r, echo = F}
ggplot(preds_5, aes(x = secchi, y = pred_secchi_5d_3m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nfive day matchups, band and 3-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_5, aes(x = secchi, y = pred_secchi_5d_5m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_5, aes(x = secchi, y = pred_secchi_5d_51m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nfive day matchups, band and 5/1-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_5, aes(x = secchi, y = pred_secchi_5d_71m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\nfive day matchups, band and 7/1-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

These look particularly poor at the higher secchi values.


### Model Performance - local knowledge window

```{r, echo = F}
ggplot(preds_jd, aes(x = secchi, y = pred_secchi_jd_3m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\n7/1 day matchups, band and 3-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_jd, aes(x = secchi, y = pred_secchi_jd_5m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\n7/1 day matchups, band and 5-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_jd, aes(x = secchi, y = pred_secchi_jd_51m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\n7/1 day matchups, band and 5/1-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

ggplot(preds_jd, aes(x = secchi, y = pred_secchi_jd_71m)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Yojoa Secchi xgboost Validation Data\n7/1 day matchups, band and 7/1-day met summaries', 
       subtitle = 'high data stringency\ngrey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

The correlations here are quite strong and linear, but they are all pretty flat and will definitely over predict low Secchi and under predict high Secchi.

# Applying model to full data

Choosing a 'best performing' model for these results is difficult. Because the predicted range is so small in the local knowledge set, I'm going to dismiss those from consideration. The 3-day window did not perform well by RMSE, but 'looked' better than the 5-day window.  With some reservation, I'm going to apply the 5-day window with 7/1-day met summaries here as the best model from this batch. This could easily be swapped with the three-day window and 5/1 day met summary.

```{r}
features = band_met71_feats
model = optimized_booster_5d_71m
met = '7/1 day met summaries'
window = '5 day window'

save(optimized_booster_5d_71m, file = 'data/models/optimized_xg_9_5d_71m.RData')

full_stack <- read_csv('data/upstreamRS/yojoa_corr_rrs_met_scaled_v2023-06-15.csv') %>%
  mutate(secchi = 100) %>%
  prepData(.) %>% 
  filter(date < ymd('2023-01-01'))

stack_xgb <- xgb.DMatrix(data = as.matrix(full_stack[,features]))

full_stack_simp <- full_stack %>%
  mutate(secchi = predict(model, stack_xgb)) %>%
  select(date, location, secchi, mission) 

situ_stack <- read_csv('data/in-situ/Secchi_completedataset.csv') %>%
  mutate(secchi = as.numeric(secchi),
         date = mdy(date)) %>%
  filter(!is.na(secchi)) %>%
  mutate(mission = 'Measured') %>%
  bind_rows(full_stack_simp)%>% 
  mutate(location = gsub(' ', '', location))
```

Let's look at each of the site records alongside the Landsat-estimated Secchi depth.

```{r, echo = F}
plotRecordBySite = function(site) {
  ggplot(situ_stack %>%
           filter(location == site), aes(x = date, y = secchi, color = mission,
                                        shape = mission)) + 
    geom_point() + 
    labs(title = paste0('Yojoa Secchi Historical Record - Site ', site),
         subtitle = paste0(window, ', ', met, ', high data stringency'),
         y = 'Secchi (m)',
         color = 'data source', shape = 'data source') +
    scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
    theme_few() +
    theme(legend.position = c(0.8,0.8)) + 
    scale_shape_manual(values = c(19,19,19,19,1)) +
    scale_y_continuous(limits = c(0, max(situ_stack$secchi)), breaks = seq(0, max(situ_stack$secchi), 2)) +
    scale_x_date(limits = c(min(situ_stack$date), max(situ_stack$date))) +
    theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
          plot.subtitle = element_text(hjust = 0.5))
}

map(sort(unique(situ_stack$location)), plotRecordBySite)
```

### Look at recent data per location

```{r}
plotRecentBySite = function(site) {
  ggplot(situ_stack %>%
           filter(location == site), aes(x = date, y = secchi, color = mission,
                                        shape = mission)) + 
    geom_point() + 
    labs(title = paste0('Yojoa Secchi 2018-2022 - Site ', site),
         subtitle = paste0(window, ', ', met, ', high data stringency'),
         y = 'Secchi (m)',
         color = 'data source', shape = 'data source') +
    scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
    theme_few() +
    theme(legend.position = c(0.8,0.8)) + 
    scale_shape_manual(values = c(19,19,19,19,1)) +
    scale_y_continuous(limits = c(0, max(situ_stack$secchi)), breaks = seq(0, max(situ_stack$secchi), 2)) +
    scale_x_date(limits = c(ymd('2018-01-01'), max(situ_stack$date))) +
    theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
          plot.subtitle = element_text(hjust = 0.5))
}

map(sort(unique(situ_stack$location)), plotRecentBySite)
```


## Whole lake secchi dynamics

While there is plenty of variability across the lake, let's summarize to a single value per date, since not all sites have the same density of record. Since there are a few oddballs in here (both in terms of measured and estimated), we'll use the median Secchi across all sites.

```{r}
lake_med <- situ_stack %>%
  group_by(date,mission) %>%
  summarize(across(where(is.numeric),median))
```

```{r, echo = F}
ggplot(lake_med, aes(x = date, y = secchi, color = mission, shape = mission)) + 
  geom_point() + 
  scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
  labs(title = 'Yojoa Secchi Historical Record\nwhole-lake median',
         subtitle = paste0(window, ', ', met, ', high data stringency'),
       y = 'median Secchi(m)',
       color = 'data source', shape = 'data source') +
  theme_few() +
  theme(legend.position = c(0.8,0.8)) + 
  scale_shape_manual(values = c(19,19,19,19,1)) +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

```

### 2006

```{r, echo = F}
ggplot(lake_med, aes(x = date, y = secchi, color = mission,shape = mission)) + 
  geom_point() + 
  scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
  theme_few() +
  labs(title = 'Yojoa Secchi 2006\nwhole-lake median',
         subtitle = paste0(window, ', ', met, ', high data stringency'),
       y = 'median Secchi (m)',
       color = 'data source', shape = 'data source') +
  theme(legend.position = c(0.8,0.8)) + 
  scale_shape_manual(values = c(19,19,19,19,1)) +
  scale_x_date(limits = c(as.Date('2006-01-01'), as.Date('2006-12-31')))+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

```

### Recent

```{r}
ggplot(lake_med, aes(x = date, y = secchi, color = mission, shape = mission)) + 
  geom_point() + 
  scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
  theme_few() +
  labs(title = 'Yojoa Secchi 2018-2022\nwhole-lake median',
         subtitle = paste0(window, ', ', met, ', high data stringency'),
       y = 'median Secchi (m)',
       color = 'data source', shape = 'data source') +
  theme(legend.position = c(0.8,0.8)) + 
  scale_shape_manual(values = c(19,19,19,19,1)) +
  scale_x_date(limits = c(as.Date('2018-01-01'), as.Date('2023-01-01')))+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

```

## Print off xgboost model feature importance

```{r}
feat_importance = xgb.importance(feature_names = band_met51_feats,
               model = optimized_booster_jd_51m)

xgb.plot.importance(feat_importance)
```


