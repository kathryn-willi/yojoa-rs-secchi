---
title: "Yojoa LS4-9 Collate/Harmonize"
author: "B Steele"
date: "2023-03-03"
output: html_document
---

# Purpose

To pull all GEE-derived data into a single dataset with scene-level metadata and save locally - this script is specific for the regional run of the GEE script that grabbed data for lake centers for all lakes \>= 25ha in Guatemala, Honduras, and El Salvador in order to create a better hand off model for all missions.

# Setup

```{r}
library(googledrive)
library(tidyverse)
library(lubridate)
library(ggthemes)

dump_dir = file.path('data/fromDrive/')
coll_dir = file.path('data/upstreamRS/')

drive_auth()
1
```

# Download and collate data and metadata from Drive

Download and collate data and metadata into separate files.

```{r}
#get a file list
files = drive_ls(path = 'yojoa', pattern = 'HON')

#function for downloading to data folder
dr_down = function(filename, fileid){
  drive_download(file = as_id(fileid), path = file.path(dump_dir, filename), overwrite = T)
}

#map over the function to download all files
map2(files$name, files$id, dr_down)

# create a list of the files in the tmp directory
list = list.files(file.path(dump_dir))
#grab only the regional files
list = list[grepl('HON', list)]
#add prefix
list = file.path(dump_dir, list)

meta_list = list[grepl('meta', list)]
data_list = list[!grepl('meta', list)]

#read them in and map to a dataframe
collated_data = map_dfr(data_list, read_csv)
collated_metadata = map_dfr(meta_list, read_csv)

#clean up workspace
rm(files)
```

Reformat the data system:index so that it will play nicely with the metadata and so we pull out the site rowid.

```{r}
grabRowid = function(sys_idx){
  parsed = str_split(sys_idx, '_')
  str_len = length(unlist(parsed))
  unlist(parsed)[str_len]
}

grabSystemIndex = function(sys_idx){
  parsed = str_split(sys_idx, '_')
  str_len = length(unlist(parsed))
  parsed_sub = unlist(parsed)[1:(str_len-1)]
  str_flatten(parsed_sub, collapse = '_')
}

collated_data$rowid = map(collated_data$`system:index`,grabRowid)
collated_data$`system:index` = map(collated_data$`system:index`, grabSystemIndex)

collated_data$`system:index` = as.character(collated_data$`system:index`)

```

Grab only the metadata we want

```{r}
filtered_metadata <- collated_metadata %>% 
  mutate(IMAGE_QUALITY = if_else(is.na(IMAGE_QUALITY), IMAGE_QUALITY_OLI, IMAGE_QUALITY)) %>% 
  select(`system:index`, 
         WRS_PATH, 
         WRS_ROW, 
         'mission' = SPACECRAFT_ID, 
         'date' = DATE_ACQUIRED, 
         'UTC_time' = SCENE_CENTER_TIME, 
         CLOUD_COVER,
         IMAGE_QUALITY, 
         IMAGE_QUALITY_TIRS, 
         SUN_AZIMUTH, 
         SUN_ELEVATION) 
  
```

Join the data and metadata.

```{r}
data = left_join(collated_data, filtered_metadata) %>% 
  mutate(rowid = as.character(rowid))

#clean up workspace
rm(collated_data, collated_metadata)

write.csv(data, file.path(coll_dir, paste0('HON_GUAT_ELSAL_regional_LandsatC2_SRST_collated_v', Sys.Date(), '.csv')))
```

## Filter scene summaries

Filter:

-   each scene-loaction must have at least 10 pixels in pCount_dswe1 (confident water) contributing to values

-   each scene must have an image quality of 7 or greater

There are likely other filters you will want to apply, but these are the minimal filters.

```{r}
filtered = data %>% 
  filter(pCount_dswe1 > 10 &
           IMAGE_QUALITY >= 7)
```

We are also going to filter to remove any median Rrs values less than -0.01 and greater than 0.2

```{r}
filtered = filtered %>% 
  filter_at(vars(med_Red, med_Green, med_Blue, med_Nir, med_Swir1, med_Swir2), all_vars(.<0.2 & .>-0.01))
```

## Export to upstreamRS folder

```{r}
write.csv(filtered, file.path(coll_dir, paste0('HON_GUAT_ELSAL_regional_LandsatC2_SRST_filtered_v', Sys.Date(), '.csv')), row.names = F)
```
